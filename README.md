# âš¡ğŸš€ CUDA Parallel Computing & GPU Programming

### ğŸ§  High-Performance Computing using CUDA Kernels & Tiled Matrix Multiplication

<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=28&duration=3000&color=00E0FF&center=true&vCenter=true&width=1000&lines=CUDA+Parallel+Computing;GPU+Kernel+Programming;High+Performance+Matrix+Multiplication;Shared+Memory+Optimization" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Platform-NVIDIA%20CUDA-green?style=for-the-badge&logo=nvidia" />
  <img src="https://img.shields.io/badge/Language-C%2FC%2B%2B-blue?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Domain-High%20Performance%20Computing-purple?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Parallelism-GPU%20Computing-orange?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Concept-Shared%20Memory-red?style=for-the-badge" />
</p>

---

# ğŸ“Œ Project Overview

This repository contains implementations of **GPU Parallel Programming using NVIDIA CUDA**, focusing on high-performance computation through kernel optimization, unified memory, and shared memory tiling techniques.

The project demonstrates how large-scale computations can be accelerated using GPU architectures instead of traditional CPU-based sequential execution.

---

# ğŸ§  Core Concepts Implemented

* CUDA Kernel Programming
* Thread & Block Hierarchy
* Unified Memory Management
* Shared Memory Optimization
* GPU vs CPU Performance Comparison
* Parallel Array Operations
* Tiled Matrix Multiplication

---

# ğŸ—ï¸ System Architecture

```
CPU (Host)
     â†“
Memory Allocation (Unified / Device Memory)
     â†“
CUDA Kernel Launch <<<Grid, Block>>>
     â†“
Parallel Execution on GPU Threads
     â†“
Synchronization & Result Verification
     â†“
Performance Metrics (Execution Time & GFLOPs)
```

---

# âš™ï¸ Implemented CUDA Modules

## 1ï¸âƒ£ Parallel Array Operations (Menu-Driven CUDA)

* Element-wise Addition Kernel
* Element-wise Subtraction Kernel
* Element-wise Multiplication Kernel
* Unified Memory Allocation (`cudaMallocManaged`)
* GPU Synchronization (`cudaDeviceSynchronize`)

Handles large arrays (1M+ elements) efficiently using GPU threads.

---

## 2ï¸âƒ£ Tiled Matrix Multiplication (Shared Memory Optimization)

* 1024 Ã— 1024 Matrix Multiplication
* Shared Memory Tiling Technique
* Reduced Global Memory Access
* CUDA Events for Kernel Timing
* Performance Analysis using GFLOPs

Optimized for high computational throughput on GPU.

---

# ğŸ”¥ Key Features

âš¡ High-Speed Parallel GPU Execution
ğŸ§® Large-Scale Matrix Computation (CUDA)
ğŸ§  Shared Memory Tiling Optimization
ğŸ“Š Kernel Performance Timing & Benchmarking
ğŸ” Menu-Driven CUDA Kernel Operations
ğŸš€ Efficient Memory Management (Unified Memory)
ğŸ§ª CPU Verification for Correctness

---

# ğŸ§© Execution Pipeline

```
Input Data â†’ Memory Allocation â†’ CUDA Kernel Launch
           â†’ Parallel GPU Computation â†’ Synchronization
           â†’ Result Copy to Host â†’ Verification & Performance Analysis
```

---

# ğŸ› ï¸ Tech Stack

| Category       | Technology                       |
| -------------- | -------------------------------- |
| Language       | C / C++ (CUDA C)                 |
| GPU Framework  | NVIDIA CUDA                      |
| Parallel Model | Grid-Block-Thread Architecture   |
| Optimization   | Shared Memory & Unified Memory   |
| Tools          | NVCC Compiler, CUDA Runtime      |
| Domain         | High Performance Computing (HPC) |

---

# ğŸ“‚ Project Structure

```
ğŸ“ CUDA-Parallel-Computing
 â”£ ğŸ“„ menu_driven_cuda.cu        # Array operations using CUDA kernels
 â”£ ğŸ“„ tiled_matrix_mul.cu        # Tiled matrix multiplication (Shared Memory)
 â”£ ğŸ“„ cuda.ipynb                 # Colab execution notebook
 â”— ğŸ“„ README.md                  # Documentation
```

---

# ğŸš€ How to Compile & Run

## ğŸ”¹ Compile using NVCC

```bash
nvcc menu_driven_cuda.cu -o menu_cuda
./menu_cuda
```

## ğŸ”¹ Run Tiled Matrix Multiplication

```bash
nvcc tiled_matrix_mul.cu -o tiled_matrix_mul
./tiled_matrix_mul
```

> Ensure CUDA Toolkit and NVIDIA GPU drivers are properly installed.

---

# ğŸ“Š Performance Highlights

* Massive speedup over CPU execution
* Efficient parallel execution using thousands of GPU threads
* Optimized global memory access via shared memory tiling
* GFLOPs calculation for performance benchmarking

---

# ğŸ¯ Academic & Industry Relevance

* High Performance Computing (HPC)
* Parallel Computing
* GPU Acceleration
* Scientific Computing
* AI/ML Infrastructure Optimization

---

# ğŸ”® Future Improvements

* CUDA Streams for Asynchronous Execution
* Multi-GPU Parallelization
* Tensor Core Optimization
* Integration with Deep Learning Workloads
* Benchmark comparison with OpenMP & CPU

---

# ğŸ‘¨â€ğŸ’» Author

**Atharva**
M.Tech CSE | VIT Vellore
HPC â€¢ CUDA â€¢ Parallel Computing â€¢ Systems Programming

---

# â­ Final Note

This project showcases practical implementation of **GPU-accelerated parallel computing using CUDA**, highlighting kernel optimization, memory management, and high-performance matrix computation techniques used in modern AI and scientific computing systems.
